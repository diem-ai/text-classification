{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %run Import_Library.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_digits_text(data):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        - Count the number of digits in each doc in data\n",
    "        - Compute the average \n",
    "    \n",
    "    Arg:\n",
    "        data: array of text\n",
    "        \n",
    "    Return: \n",
    "        an average of number of digits in data\n",
    "    \"\"\"\n",
    "    avg = []\n",
    "    num_re = re.compile('[0-9]')\n",
    "    for text in data:\n",
    "        match = num_re.findall(text)\n",
    "        if(len(match) > 0):\n",
    "            avg.append(len(match))\n",
    "            \n",
    "    return np.mean(avg)\n",
    "\n",
    "\n",
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Combine new features into the sparse matrix\n",
    "    \n",
    "    Return:\n",
    "        sparse feature matrix with added feature.\n",
    "\n",
    "    Arg:\n",
    "     - X: sparse feature matrix, for example:\n",
    "         array([[   0.,    0.,    0., ...,    0.,    0.,   31.],\n",
    "           [   0.,    0.,    0., ...,    0.,    0.,  130.],\n",
    "           [   0.,    0.,    0., ...,    0.,    0.,   66.],\n",
    "           ..., \n",
    "           [   0.,    0.,    0., ...,    0.,    0.,  147.],\n",
    "           [   0.,    0.,    0., ...,    0.,    0.,   62.],\n",
    "           [   0.,    0.,    0., ...,    0.,    0.,   82.]])\n",
    "       \n",
    "     - feature_to_add: list of features, for example:\n",
    "         [[ 31, 130,  66, ..., 147,  62,  82]]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove    characters '"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove alpha numerical words and make lowercase\n",
    "def remove_non_alphameric_character(text):\n",
    "    non_alpha_char_re = re.compile('[^a-zA-Z0-9 ]')\n",
    "    return non_alpha_char_re.sub('', text.strip().lower())\n",
    "\n",
    "'''\n",
    "# make a test\n",
    "text = 'remoVe $ Â£ # characters _'\n",
    "remove_non_alphameric_character(text)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation characters found: ['!', '!', '!', ',', '#', '#', '$', '$', '.'] in the text: Hello!!!, This is ##STechies$$.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello This is STechies'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    return punc_re.sub('', text)\n",
    "\n",
    "'''\n",
    "# make a test\n",
    "my_string = \"Hello!!!, This is ##STechies$$.\"\n",
    "punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "print(\"Punctuation characters found:\", punc_re.findall(my_string), \"in the text:\", my_string)\n",
    "remove_punctuation(my_string)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How many stop words sentence ?'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words\n",
    "def remove_stopwords(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return ' '.join([word for word in word_tokenize(text) if word not in stop_words])\n",
    "\n",
    "'''\n",
    "# make a test\n",
    "text = \"How many are there stop words in this sentence ? \"\n",
    "remove_stopwords(text)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Convert the part-of-speech naming scheme\n",
    "    from the nltk default to that which is\n",
    "    recognized by the WordNet lemmatizer\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before the lemmatization: Where multiple candidates for the LCS exist, that whose shortest path to the root node is the longest will be selected \n",
      "After the lemmatization: Where multiple candidate for the LCS exist , that whose short path to the root node be the long will be select\n"
     ]
    }
   ],
   "source": [
    "def lemmatizer(text):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Tokenize the text and mark the part of speed (pos) tagging for each token\n",
    "    word_pos = nltk.pos_tag(word_tokenize(text))\n",
    "    # Get the part-of-speech of each token in text\n",
    "    pos_tags = [get_wordnet_pos(pair[1]) for pair in word_pos]\n",
    "    # get the lemma of each token according to their position tag\n",
    "    return ' '.join([lemmatizer.lemmatize(pair[0], pair[1]) for pair in zip(word_tokenize(text), pos_tags)])\n",
    "\n",
    "'''\n",
    "# make a test\n",
    "text = \"Where multiple candidates for the LCS exist, that whose shortest path to the root node is the longest will be selected \"\n",
    "print(\"Before the lemmatization:\", text)\n",
    "print(\"After the lemmatization:\", lemmatizer(text))\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = remove_non_alphameric_character(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatizer(text)\n",
    "    return text\n",
    "\n",
    "# make a test\n",
    "# make a test\n",
    "text = \"Where multiple candidates for the LCS exist, that whose shortest path to the root node is the longest will be selected \"\n",
    "print(\"Before preprocessing():\", text)\n",
    "print(\"After preprocessing():\", preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Make a test\\ntext = \"In this tutorial, you will learn about Nltk FreqDist function with example. This function is used to find the frequency of words within a text.\"\\nword_frequency(text.lower())\\n'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_frequency(text):\n",
    "    '''\n",
    "    Count the frequency of each word in given text (ignore case)\n",
    "    Return a dictionary whose key is token and its frequency\n",
    "    '''\n",
    "    \n",
    "    word_freq = {}\n",
    "    text = text.lower()\n",
    "    for word in word_tokenize(text):\n",
    "        if word in word_freq:\n",
    "            word_freq[word] =  word_freq[word] + 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "    return word_freq\n",
    "\n",
    "'''\n",
    "\n",
    "# Make a test\n",
    "text = \"In this tutorial, you will learn about Nltk FreqDist function with example. This function is used to find the frequency of words within a text.\"\n",
    "word_frequency(text.lower())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 2),\n",
       " ('function', 2),\n",
       " ('.', 2),\n",
       " ('in', 1),\n",
       " ('tutorial', 1),\n",
       " (',', 1),\n",
       " ('you', 1),\n",
       " ('will', 1),\n",
       " ('learn', 1),\n",
       " ('about', 1),\n",
       " ('nltk', 1),\n",
       " ('freqdist', 1),\n",
       " ('with', 1),\n",
       " ('example', 1),\n",
       " ('is', 1),\n",
       " ('used', 1),\n",
       " ('to', 1),\n",
       " ('find', 1),\n",
       " ('the', 1),\n",
       " ('frequency', 1)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_word_frequency(text, n=10):\n",
    "    \n",
    "    word_freq = word_frequency(text)\n",
    "    # sort the dictionary with descending by setting reverse=False\n",
    "    # because by default, sorted() will sort the dict() with ascending order\n",
    "    return sorted( word_freq.items(), key = lambda item : item[1], reverse = True)[:n]\n",
    "\n",
    "'''\n",
    "# Make a test\n",
    "text = \"In this tutorial, you will learn about Nltk FreqDist function with example. This function is used to find the frequency of words within a text.\"\n",
    "top_word_frequency(text, 20)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99641577, 0.99103945, 0.98566308, 0.98563734, 0.99641588, 0.99103943]\n",
      "[4, 5, 2, 6, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def myshuffle(seed, data):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(data)\n",
    "\n",
    "# Make a test\n",
    "\n",
    "X = [0.99103943, 0.98566308, 0.99641588, 0.99641577, 0.99103945, 0.98563734]\n",
    "\n",
    "Y = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "myshuffle(123, X)\n",
    "\n",
    "myshuffle(123, Y)\n",
    "\n",
    "print(X)\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k = min(TOP_K, x_train.shape[1]))\n",
    "    \n",
    "    selector.fit(x_train, train_labels)\n",
    "    \n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    \n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    \n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Import Accessory_Functions.ipynb : DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
